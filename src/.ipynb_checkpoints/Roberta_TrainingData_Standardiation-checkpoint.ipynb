{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unable to import 'smart_open.gcs', disabling that module\n"
     ]
    }
   ],
   "source": [
    "from simpletransformers.classification import ClassificationModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from collections import Counter\n",
    "from uuid import uuid4\n",
    "from random import seed\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from random import randrange\n",
    "from sklearn.utils import shuffle\n",
    "import re\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from gensim.parsing.preprocessing import preprocess_documents, strip_tags,strip_punctuation, strip_numeric, remove_stopwords, strip_short, stem_text\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import unidecode\n",
    "import unicodedata\n",
    "import re\n",
    "import unidecode\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from gensim.parsing.preprocessing import preprocess_documents, strip_tags,strip_punctuation, strip_numeric, remove_stopwords, strip_short, stem_text,strip_multiple_whitespaces\n",
    "import gc\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_trian = pd.read_pickle(\"/home/ec2-user/surya/data/train_more_data/data2_clean_dupContent_NEW_APR172020.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new_trian.label = np.where(data_new_trian.label == 'operation', 'operations',data_new_trian.label )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tax                      5436\n",
       "operations               4657\n",
       "regulatory               4045\n",
       "insurance                3838\n",
       "intellectual property    2712\n",
       "contracts                2312\n",
       "legal                    1991\n",
       "sales and marketing      1970\n",
       "product                  1851\n",
       "technical and it         1665\n",
       "human resources          1625\n",
       "financial                1495\n",
       "corporate                1476\n",
       "production               1097\n",
       "environmental            1084\n",
       "real estate                88\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new_trian.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessFlag = True\n",
    "# if preprocessFlag == True:\n",
    "#     data2.label = data2.label.apply(lambda x : x.lower())\n",
    "#     data2.label = np.where(data2.label=='sales','sales and marketing', data2.label)\n",
    "#     data2.label = np.where(data2.label=='marketing','sales and marketing', data2.label)\n",
    "#     data2.label = np.where(data2.label=='litigation','legal', data2.label)\n",
    "#     data2.label = np.where(data2.label=='eh&s','environmental', data2.label)\n",
    "#     data2.label = np.where(data2.label=='it','technical and it', data2.label)\n",
    "#     data2.label = np.where(data2.label=='technical','technical and it', data2.label)\n",
    "#     data2.label = np.where(data2.label=='sales_and_marketing','sales and marketing', data2.label)\n",
    "#     data2.label = np.where(data2.label=='ip','intellectual property', data2.label)\n",
    "#     data2.label = np.where(data2.label=='hr','human resources', data2.label)\n",
    "#     data2.label = np.where(data2.label=='real property','real estate', data2.label)\n",
    "\n",
    "\n",
    "#     data2_test.label = data2_test.label.apply(lambda x : x.lower())\n",
    "#     data2_test.label = np.where(data2_test.label=='sales','sales and marketing', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='marketing','sales and marketing', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='litigation','legal', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='eh&s','environmental', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='it','technical and it', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='technical','technical and it', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='sales_and_marketing','sales and marketing', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='ip','intellectual property', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='hr','human resources', data2_test.label)\n",
    "#     data2_test.label = np.where(data2_test.label=='real property','real estate', data2_test.label)\n",
    "\n",
    "# data3 = data2[(data2.clean_content_v1.str.contains('could_not_read')) | (data2.clean_content_v1.str.contains('confidential siddhartha allen')) | \n",
    "#               (data2.clean_content_v1.str.contains(\"this file could not be converted to secure file format please notify the workspace owner you may be able to view the file online by clicking preview inside the workspace web page\"))]\n",
    "# data2 = data2.drop(data3.index).reset_index().drop('index', axis =1)\n",
    "\n",
    "# data3_test = data2_test[(data2_test.clean_content_v1.str.contains('could_not_read')) | (data2.clean_content_v1.str.contains('confidential siddhartha allen')) | \n",
    "#               (data2_test.clean_content_v1.str.contains(\"this file could not be converted to secure file format please notify the workspace owner you may be able to view the file online by clicking preview inside the workspace web page\"))]\n",
    "# data2_test = data2_test.drop(data3_test.index).reset_index().drop('index', axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tax                      5436\n",
       "operations               4657\n",
       "regulatory               4045\n",
       "insurance                3838\n",
       "intellectual property    2712\n",
       "contracts                2312\n",
       "legal                    1991\n",
       "sales and marketing      1970\n",
       "product                  1851\n",
       "technical and it         1665\n",
       "human resources          1625\n",
       "financial                1495\n",
       "corporate                1476\n",
       "production               1097\n",
       "environmental            1084\n",
       "real estate                88\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new_trian.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37342"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new_trian.label.value_counts().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_trim</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>type</th>\n",
       "      <th>label_len</th>\n",
       "      <th>type_len</th>\n",
       "      <th>type_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>please remit to lark learn not to burn washing...</td>\n",
       "      <td>corporate</td>\n",
       "      <td>/Users/sallen/Desktop/AI/DealVision/Model/RUBI...</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sid_vdr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pic resource unit definitions schedule annex e...</td>\n",
       "      <td>contracts</td>\n",
       "      <td>/Users/sallen/Desktop/AI/DealVision/Model/RUBI...</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sid_vdr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pic security compliance policy schedule entere...</td>\n",
       "      <td>contracts</td>\n",
       "      <td>/Users/sallen/Desktop/AI/DealVision/Model/RUBI...</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sid_vdr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please read this trademark usage policy this p...</td>\n",
       "      <td>intellectual property</td>\n",
       "      <td>Intellectual Property/Trademarks &amp; Trade Names...</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>public</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pic rumor and gossip can be costly in more way...</td>\n",
       "      <td>regulatory</td>\n",
       "      <td>/Users/sallen/Desktop/AI/DealVision/Model/RUBI...</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sid_vdr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        content_trim                  label  \\\n",
       "0  please remit to lark learn not to burn washing...              corporate   \n",
       "1  pic resource unit definitions schedule annex e...              contracts   \n",
       "2  pic security compliance policy schedule entere...              contracts   \n",
       "3  please read this trademark usage policy this p...  intellectual property   \n",
       "4  pic rumor and gossip can be costly in more way...             regulatory   \n",
       "\n",
       "                                                path     type  label_len  \\\n",
       "0  /Users/sallen/Desktop/AI/DealVision/Model/RUBI...  sid_vdr          1   \n",
       "1  /Users/sallen/Desktop/AI/DealVision/Model/RUBI...  sid_vdr          1   \n",
       "2  /Users/sallen/Desktop/AI/DealVision/Model/RUBI...  sid_vdr          1   \n",
       "3  Intellectual Property/Trademarks & Trade Names...   public          1   \n",
       "4  /Users/sallen/Desktop/AI/DealVision/Model/RUBI...  sid_vdr          1   \n",
       "\n",
       "   type_len    type_  \n",
       "0         1  sid_vdr  \n",
       "1         1  sid_vdr  \n",
       "2         1  sid_vdr  \n",
       "3         1   public  \n",
       "4         1  sid_vdr  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new_trian.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# data1 =  pd.read_pickle(\"/content/drive/My Drive/ClassifierData_DocTitles/real_estate_preprocessed.pickle\")\n",
    "# data2 = pd.read_pickle(\"/content/drive/My Drive/ClassifierData_DocTitles/operations_preprocessed.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langdetect import detect\n",
    "# data1[\"lang\"] = data1.clean_content_v1.apply(detect)\n",
    "\n",
    "# data2 = data2[data2.clean_content_v1_len > 10]\n",
    "\n",
    "# data2[\"lang\"] = data2.clean_content_v1.apply(detect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data1 = data1[data1[\"lang\"] == \"en\"].reset_index(drop=True)\n",
    "# data2 = data2[data2[\"lang\"] == \"en\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = pd.DataFrame()\n",
    "data_[\"text\"] = data1.clean_content_v1\n",
    "data_[\"label\"] = \"real estate\"\n",
    "\n",
    "data1_ = pd.DataFrame()\n",
    "data1_[\"text\"] = data2.clean_content_v1\n",
    "data1_[\"label\"] = \"operations\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ = data_[~data_.text.str.contains(\"confidential siddhartha allen intralinks_ny\")].reset_index(drop=True)\n",
    "data1_ = data1_[~data1_.text.str.contains(\"condential confidential theodore siddharth\")].reset_index(drop=True)\n",
    "data_ = data_[~data_.text.str.contains(\"confidential siddhartha allen intralinks_ny\")].drop_duplicates().reset_index(drop=True)\n",
    "data1_ = data1_[~data1_.text.str.contains(\"confidential siddhartha allen intralinks_ny\")].drop_duplicates().reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_to_run = pd.concat([data_,data1_,data_new_trian[[\"content_trim\",\"label\"]].rename(columns = {\"content_trim\":\"text\"})], axis =0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_run = data_new_trian.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Classifier Code ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "junk_words = ['file', 'noname' , 'pdf' , 'file' , 'jpg' , 'jpeg' , '', 'doc' , 'docx' , 'xls' , 'xlsx' , 'https' , 'unnamed' , 'page' ,'of' ,'htm', 'http', 'www']\n",
    "\n",
    "junk_words = junk_words\n",
    "\n",
    "def limitContentLength(sContent, max_length = 512, max_split=True):\n",
    "    '''set string x to fixed_n character, prepend with 'xxx' if short'''\n",
    "    x = sContent.split()\n",
    "    x = [x1 for x1 in x if x1 not in junk_words]\n",
    "    if len(x) > max_length: \n",
    "        return ' '.join(x[:max_length])\n",
    "    return sContent\n",
    "def remove_extensions(doc):\n",
    "  return ' '.join([doc for doc in doc.split() if doc.lower() not in junk_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_k_elements(group, k=1000):\n",
    "    if len(group) < k:\n",
    "        return group\n",
    "    return group.sample(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tax                      5436\n",
       "operations               4657\n",
       "regulatory               4045\n",
       "insurance                3838\n",
       "intellectual property    2712\n",
       "contracts                2312\n",
       "legal                    1991\n",
       "sales and marketing      1970\n",
       "product                  1851\n",
       "technical and it         1665\n",
       "human resources          1625\n",
       "financial                1495\n",
       "corporate                1476\n",
       "production               1097\n",
       "environmental            1084\n",
       "real estate                88\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_new_trian.label = np.where(data_new_trian.label == 'operation', 'operations',data_new_trian.label )\n",
    "data_new_trian.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(test_y, predict_y):\n",
    "    C = confusion_matrix(test_y, predict_y)\n",
    "    A =(((C.T)/(C.sum(axis=1))).T)\n",
    "    B =(C/C.sum(axis=0))\n",
    "    labels = [1,2,3,4,5]\n",
    "    print(\"-\"*40, \"Confusion matrix\", \"-\"*40)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(C, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"-\"*40, \"Precision matrix \", \"-\"*40)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(B, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()\n",
    "    \n",
    "    # representing B in heatmap format\n",
    "    print(\"-\"*40, \"Recall matrix \", \"-\"*40)\n",
    "    plt.figure(figsize=(20,7))\n",
    "    sns.heatmap(A, annot=True, cmap=\"YlGnBu\", fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted Class')\n",
    "    plt.ylabel('Original Class')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def into_clean_string(txt):\n",
    "    return ' '.join(gensim.utils.simple_preprocess(txt))\n",
    "\n",
    "\n",
    "def get_class_weights(y):\n",
    "    counter = Counter(y)\n",
    "    majority = max(counter.values())\n",
    "    return {cls: round(float(majority) / float(count), 2) for cls, count in counter.items()}\n",
    "\n",
    "\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Actual Document Classiifer Starts!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "MODEL_BASE = '/home/ec2-user/surya/DocClassifier_sep24/'\n",
    "\n",
    "def create_ovr_calssifier_with_imbalance(MODEL_BASE_DIRECTORY_TO_USE, data_to_eval  , train_args , columnName = None):\n",
    "  num_labels = len(data_to_eval.label.unique())\n",
    "  print(num_labels)\n",
    "  # model_to_use = ClassificationModel('roberta', 'roberta-large',num_labels=num_labels, args=train_args)\n",
    "\n",
    "  # data_to_eval = pd.DataFrame()\n",
    "\n",
    "  print(data_to_eval.columns)\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(data_to_eval[\"text\"],\n",
    "                                                        data_to_eval[\"label\"], \n",
    "                                                        stratify=data_to_eval[\"label\"],shuffle= True,\n",
    "                                                        test_size=0.20, random_state=42,)\n",
    "  train_df = pd.DataFrame()\n",
    "  train_df[\"text\"] = X_train\n",
    "  train_df[\"label\"] = y_train\n",
    "\n",
    "  train_df.to_csv(\"train_df_to_validate.csv\", index=False, encoding='utf-8')\n",
    "\n",
    "  class_weight_dict = get_class_weights(train_df.label)\n",
    "\n",
    "  # print('Counter Function: ',class_weight_dict)\n",
    "\n",
    "  # class_weight_dict = [class_weight_dict[0],class_weight_dict[1]]\n",
    "  # print(type(class_weight_dict) , class_weight_dict)\n",
    "\n",
    "  \n",
    "\n",
    "  classes_ = np.array(train_df[\"label\"].unique())\n",
    "  print(\"classes_\",classes_)\n",
    "\n",
    "  class_weight_list  = class_weight.compute_class_weight(class_weight='balanced', classes = classes_,y=train_df.label)\n",
    "  print('class_weight Function' , class_weight_list)\n",
    "\n",
    "  # class_weight_dict = class_weight_list\n",
    "  print('class_weight_dict to input',class_weight_list)\n",
    "  model_to_use = ClassificationModel('roberta', 'roberta-large', num_labels=num_labels, args=train_args)\n",
    "  # model_to_use = ClassificationModel('roberta', '/content/drive/My Drive/RobertA_TrainingData/OVR_MULTI_Converged_Classifer_MAY17/_MULTI_/best_model')\n",
    "  \n",
    "  print(\"model_to_use Robert loaded from prettrained\")\n",
    "  print(\"num_labels num_labels num_labels:\",num_labels)\n",
    "  \n",
    "  # model_to_use = ClassificationModel('roberta', 'roberta-large', num_labels=num_labels, args=train_args)\n",
    "\n",
    "  # model_to_use = ClassificationModel('roberta', 'roberta-large',weight = class_weight_list , num_labels=num_labels, args=train_args)\n",
    "\n",
    "  eval_df = pd.DataFrame()\n",
    "  eval_df[\"text\"] = X_test\n",
    "  eval_df[\"label\"] = y_test\n",
    "  \n",
    "  \n",
    "  \n",
    "  model_to_use.train_model(train_df, show_running_loss=False,eval_df=eval_df, args={'auto_weights':True, 'show_running_loss' : False})\n",
    "  \n",
    "  # model_to_use =  ClassificationModel('roberta', '/content/drive/My Drive/RobertA_TrainingData/OVR_MULTI_Converged_Classifer/_MULTI_/best_model')\n",
    "  train_result, train_model_outputs,train_wrong_predictions = model_to_use.eval_model(train_df, acc=sklearn.metrics.accuracy_score)\n",
    "  print(\"train_result\", train_result)\n",
    "  eval_result, model_outputs, wrong_predictions = model_to_use.eval_model(eval_df,acc=sklearn.metrics.accuracy_score)\n",
    "\n",
    "\n",
    "  print(\"eval_result\", eval_result)\n",
    "  return model_to_use,eval_result,eval_df\n",
    "  \n",
    "\n",
    "\n",
    "def ovr_document_content_classifier(data2):\n",
    "    data_2_classify  = data2.copy()\n",
    "    num_of_labels = len(np.unique(data2.label))\n",
    "    print(\"num_of_labels : \",num_of_labels)\n",
    "    \n",
    "    # indexes = data_2_classify[data_2_classify.label == label_name].index\n",
    "    # data_2_classify.loc[indexes,'label_enc' ] =1 \n",
    "    # data_2_classify.loc[~data_2_classify.index.isin(indexes) , 'label_enc'] =0\n",
    "    foldername = '_MULTI_'\n",
    "    MODEL_BASE_DIRECTORY_TO_USE = MODEL_BASE\n",
    "    best_model_dir = MODEL_BASE +foldername+ \"/best_model/\"\n",
    "    tf_dir = MODEL_BASE + foldername+'/runs'\n",
    "    cache_dir = MODEL_BASE+foldername+'/cache'\n",
    "   \n",
    "    multilabel_train_args_old = {\n",
    "    'evaluate_during_training': True,\n",
    "    \"cache_dir\": cache_dir,\n",
    "    'best_model_dir': best_model_dir,\n",
    "    'tensorboard_dir' : tf_dir,\n",
    "    'logging_steps': 300,\n",
    "    'overwrite_output_dir': True,\n",
    "    'max_seq_length' : 512,\n",
    "    'evaluate_during_training_steps' :300,\n",
    "    \"evaluate_during_training_verbose\": True,\n",
    "    'reprocess_input_data': True,\n",
    "    'train_batch_size': 2,\n",
    "    'use_cached_eval_features': True,\n",
    "    'no_cache' : False,\n",
    "    'do_lower_case' :True,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    'output_dir':MODEL_BASE_DIRECTORY_TO_USE,\n",
    "    'save_eval_checkpoints' : False,\n",
    "    'gradient_accumulation_steps':8,  ## TO uNcomment Out for Operations are done!!!\n",
    "    # \"warmup_ratio\": 0.06,\n",
    "    'weight_decay' : 0.1,\n",
    "    # 'learning_rate' : 1e-5,\n",
    "    'learning_rate' : 4e-5,\n",
    "    'num_train_epochs' : 3,\n",
    "    'use_early_stopping' : True,\n",
    "    'early_stopping_patience' : 5\n",
    "    }\n",
    "\n",
    "    multilabel_train_args_NEW = {\n",
    "    'evaluate_during_training': True,\n",
    "    \"cache_dir\": cache_dir,\n",
    "    'best_model_dir': best_model_dir,\n",
    "    'tensorboard_dir' : tf_dir,\n",
    "    'logging_steps': 200,\n",
    "    'overwrite_output_dir': True,\n",
    "    'max_seq_length' : 512,\n",
    "    'evaluate_during_training_steps' :100,\n",
    "    \"evaluate_during_training_verbose\": True,\n",
    "    'reprocess_input_data': True,\n",
    "    'train_batch_size': 4,\n",
    "    'use_cached_eval_features': True,\n",
    "    'no_cache' : False,\n",
    "    'do_lower_case' :False,\n",
    "    \"adam_epsilon\": 1e-8,\n",
    "    'output_dir':MODEL_BASE_DIRECTORY_TO_USE,\n",
    "    'save_eval_checkpoints' : False,\n",
    "    'gradient_accumulation_steps':8,\n",
    "    # \"warmup_ratio\": 0.06,\n",
    "    # 'weight_decay' : 0.1,\n",
    "    'learning_rate' : 1e-5,\n",
    "    'num_train_epochs' : 1,\n",
    "    'use_early_stopping' : True,\n",
    "    'early_stopping_patience' : 5\n",
    "    }\n",
    "\n",
    "    model_to_use,eval_result,eval_df = create_ovr_calssifier_with_imbalance(MODEL_BASE_DIRECTORY_TO_USE,\n",
    "                                                        data_2_classify,\n",
    "                                                        multilabel_train_args_old,\n",
    "                                                        columnName = \"text\" )\n",
    "    # print('eval_result')\n",
    "    # print(eval_result)\n",
    "    # print('eval_df.label') \n",
    "    # print(eval_df.label)    \n",
    "    # plot_confusion_matrix( eval_df.label,eval_result)\n",
    "    # col_name = foldername\n",
    "\n",
    "    # print(\"OVR Content CLassifer \",col_name,\"@ : \",MODEL_BASE_DIRECTORY_TO_USE)\n",
    "    # # inferOneVsRestClassifer(model_to_use,col_name, MODEL_BASE_DIRECTORY_TO_USE ,MODEL_BASE, train_args=multilabel_train_args)\n",
    "    # inferOneVsRestClassifer_allvdrs(model_to_use,col_name, MODEL_BASE_DIRECTORY_TO_USE ,MODEL_BASE, train_args=multilabel_train_args)\n",
    "    \n",
    "    return model_to_use,eval_result,eval_df\n",
    "    # inferOneVsRestClassifer_allvdrs_titles\n",
    "    # del model_to_use\n",
    "    # del data_2_classify\n",
    "\n",
    "    # gc.collect()\n",
    "    # torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Sep 25 09:30:57 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 00000000:00:1E.0 Off |                    0 |\r\n",
      "| N/A   53C    P8    28W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib,pickle\n",
    "filename = '/home/ec2-user/surya/DocClassifier_sep24/doc_labelencoder.pkl'\n",
    "# filename = 'doc_labelencoder.pkl'\n",
    "\n",
    "joblib.dump(labelencoder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = data_to_run.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['content_trim', 'label', 'path', 'type', 'label_len', 'type_len',\n",
      "       'type_', 'label_enc'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "labelencoder = LabelEncoder()\n",
    "data2[\"label_enc\"] = labelencoder.fit_transform(data2.label)\n",
    "# data3 = data2[[\"content_trim\",\"label_enc\"]].rename(columns = {\"content_trim\" : \"text\", \"label_enc\":\"label\"})\n",
    "data3 = data2.copy()\n",
    "print(data3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data3.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['contracts', 'corporate', 'environmental', 'financial',\n",
       "        'human resources', 'insurance', 'intellectual property', 'legal',\n",
       "        'operations', 'product', 'production', 'real estate', 'regulatory',\n",
       "        'sales and marketing', 'tax', 'technical and it'], dtype=object),\n",
       " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder.classes_, labelencoder.fit_transform(labelencoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/ec2-user/surya/DocClassifier_sep24/doc_labelencoder.pkl']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib,pickle\n",
    "filename = '/home/ec2-user/surya/DocClassifier_sep24/doc_labelencoder.pkl'\n",
    "# filename = 'doc_labelencoder.pkl'\n",
    "\n",
    "joblib.dump(labelencoder, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tax                      5436\n",
       " operations               4657\n",
       " regulatory               4045\n",
       " insurance                3838\n",
       " intellectual property    2712\n",
       " contracts                2312\n",
       " legal                    1991\n",
       " sales and marketing      1970\n",
       " product                  1851\n",
       " technical and it         1665\n",
       " human resources          1625\n",
       " financial                1495\n",
       " corporate                1476\n",
       " production               1097\n",
       " environmental            1084\n",
       " real estate                88\n",
       " Name: label, dtype: int64,\n",
       " Index(['content_trim', 'label', 'path', 'type', 'label_len', 'type_len',\n",
       "        'type_', 'label_enc'],\n",
       "       dtype='object'))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.label.value_counts(),data3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content_trim</th>\n",
       "      <th>label</th>\n",
       "      <th>path</th>\n",
       "      <th>type</th>\n",
       "      <th>label_len</th>\n",
       "      <th>type_len</th>\n",
       "      <th>type_</th>\n",
       "      <th>label_enc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>please remit to lark learn not to burn washing...</td>\n",
       "      <td>corporate</td>\n",
       "      <td>/Users/sallen/Desktop/AI/DealVision/Model/RUBI...</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pic resource unit definitions schedule annex e...</td>\n",
       "      <td>contracts</td>\n",
       "      <td>/Users/sallen/Desktop/AI/DealVision/Model/RUBI...</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pic security compliance policy schedule entere...</td>\n",
       "      <td>contracts</td>\n",
       "      <td>/Users/sallen/Desktop/AI/DealVision/Model/RUBI...</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>please read this trademark usage policy this p...</td>\n",
       "      <td>intellectual property</td>\n",
       "      <td>Intellectual Property/Trademarks &amp; Trade Names...</td>\n",
       "      <td>public</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>public</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pic rumor and gossip can be costly in more way...</td>\n",
       "      <td>regulatory</td>\n",
       "      <td>/Users/sallen/Desktop/AI/DealVision/Model/RUBI...</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>sid_vdr</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        content_trim                  label  \\\n",
       "0  please remit to lark learn not to burn washing...              corporate   \n",
       "1  pic resource unit definitions schedule annex e...              contracts   \n",
       "2  pic security compliance policy schedule entere...              contracts   \n",
       "3  please read this trademark usage policy this p...  intellectual property   \n",
       "4  pic rumor and gossip can be costly in more way...             regulatory   \n",
       "\n",
       "                                                path     type  label_len  \\\n",
       "0  /Users/sallen/Desktop/AI/DealVision/Model/RUBI...  sid_vdr          1   \n",
       "1  /Users/sallen/Desktop/AI/DealVision/Model/RUBI...  sid_vdr          1   \n",
       "2  /Users/sallen/Desktop/AI/DealVision/Model/RUBI...  sid_vdr          1   \n",
       "3  Intellectual Property/Trademarks & Trade Names...   public          1   \n",
       "4  /Users/sallen/Desktop/AI/DealVision/Model/RUBI...  sid_vdr          1   \n",
       "\n",
       "   type_len    type_  label_enc  \n",
       "0         1  sid_vdr          1  \n",
       "1         1  sid_vdr          0  \n",
       "2         1  sid_vdr          0  \n",
       "3         1   public          6  \n",
       "4         1  sid_vdr         12  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data3[[\"content_trim\",\"label_enc\"]].rename(columns= {\"content_trim\" : \"text\", \"label_enc\":\"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_labels :  16\n",
      "16\n",
      "Index(['text', 'label'], dtype='object')\n",
      "classes_ [13  1 15  9 14  6  8  7 10  4 12  5  0  2  3 11]\n",
      "class_weight Function [ 1.18468433  1.5809166   1.40169857  1.26067691  0.42930846  0.86079414\n",
      "  0.50122483  1.17204175  2.12649487  1.43620192  0.57696616  0.60816368\n",
      "  1.00922297  2.15347463  1.56108905 26.67232143]\n",
      "class_weight_dict to input [ 1.18468433  1.5809166   1.40169857  1.26067691  0.42930846  0.86079414\n",
      "  0.50122483  1.17204175  2.12649487  1.43620192  0.57696616  0.60816368\n",
      "  1.00922297  2.15347463  1.56108905 26.67232143]\n",
      "model_to_use Robert loaded from prettrained\n",
      "num_labels num_labels num_labels: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/rubic_model/lib/python3.7/site-packages/simpletransformers/classification/classification_model.py:251: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  \"Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d68247b1ecd4aac865cc2c07e55f39b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=29873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Warning:  multi_tensor_applier fused unscale kernel is unavailable, possibly because apex was installed without --cuda_ext --cpp_ext. Using Python fallback.  Original ImportError was: ModuleNotFoundError(\"No module named 'amp_C'\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a13a2b06754acc9fb1f18a523c9c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f29687f465984e60b37e0d4b7284d787",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Current iteration', max=14937.0, style=ProgressStyle(desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/rubic_model/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:113: UserWarning: Seems like `optimizer.step()` has been overridden after learning rate scheduler initialization. Please, make sure to call `optimizer.step()` before `lr_scheduler.step()`. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n"
     ]
    }
   ],
   "source": [
    "model_to_use,eval_result,eval_df = ovr_document_content_classifier(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds, model_outputs = model_to_use.predict(eval_df.text.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = labelencoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(act,preds,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
